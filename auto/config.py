"""
ëª¨ë¸ ì„¤ì • íŒŒì¼
ë‹¤ì–‘í•œ LLM ì œê³µìì™€ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
"""

import os
from dotenv import load_dotenv

load_dotenv()

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
MODELS = {
    # OpenAI ëª¨ë¸ë“¤
    "gpt-4o": {
        "provider": "openai",
        "model": "gpt-4o",
        "supports_functions": True,
        "cost": "high"
    },
    "gpt-4o-mini": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "supports_functions": True,
        "cost": "medium"
    },
    "gpt-3.5-turbo": {
        "provider": "openai",
        "model": "gpt-3.5-turbo",
        "supports_functions": True,
        "cost": "low"
    },

    # Anthropic Claude (langchain-anthropic ì„¤ì¹˜ í•„ìš”)
    "claude-3-opus": {
        "provider": "anthropic",
        "model": "claude-3-opus-20240229",
        "supports_functions": True,
        "cost": "high"
    },
    "claude-3-sonnet": {
        "provider": "anthropic",
        "model": "claude-3-sonnet-20240229",
        "supports_functions": True,
        "cost": "medium"
    },

    # Google Gemini (langchain-google-genai ì„¤ì¹˜ í•„ìš”)
    "gemini-pro": {
        "provider": "google",
        "model": "gemini-pro",
        "supports_functions": False,
        "cost": "low"
    },
    "gemini-2.5-flash": {
        "provider": "google",
        "model": "gemini-2.5-flash",
        "supports_functions": False,
        "cost": "low"
    },


    # Ollama ë¡œì»¬ ëª¨ë¸ (langchain-community ì„¤ì¹˜ í•„ìš”)
    "gpt-oss": {
        "provider": "ollama",
        "model": "gpt-oss:20b",
        "supports_functions": False,
        "cost": "free"
    },
    "llama2": {
        "provider": "ollama",
        "model": "llama2",
        "supports_functions": False,
        "cost": "free"
    },
    "mistral": {
        "provider": "ollama",
        "model": "mistral",
        "supports_functions": False,
        "cost": "free"
    },
    "gemma": {
        "provider": "ollama",
        "model": "gemma",
        "supports_functions": False,
        "cost": "free"
    }
}

def get_llm(model_name="gpt-3.5-turbo", temperature=0.7):
    """
    ì„ íƒí•œ ëª¨ë¸ì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Args:
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        temperature: ìƒì„± ì˜¨ë„ (0~1)

    Returns:
        LLM ì¸ìŠ¤í„´ìŠ¤
    """

    if model_name not in MODELS:
        print(f"âš ï¸ {model_name}ì€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. gpt-3.5-turboë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        model_name = "gpt-3.5-turbo"

    model_info = MODELS[model_name]
    provider = model_info["provider"]

    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=model_info["model"],
            temperature=temperature,
            api_key=os.getenv("OPENAI_API_KEY")
        )

    elif provider == "anthropic":
        # pip install langchain-anthropic í•„ìš”
        try:
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(
                model=model_info["model"],
                temperature=temperature,
                api_key=os.getenv("ANTHROPIC_API_KEY")
            )
        except ImportError:
            print("âš ï¸ langchain-anthropicì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜: pip install langchain-anthropic")
            return get_llm("gpt-3.5-turbo", temperature)

    elif provider == "google":
        # pip install langchain-google-genai í•„ìš”
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            return ChatGoogleGenerativeAI(
                model=model_info["model"],
                temperature=temperature,
                google_api_key=os.getenv("GOOGLE_API_KEY")
            )
        except ImportError:
            print("âš ï¸ langchain-google-genaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜: pip install langchain-google-genai")
            return get_llm("gpt-3.5-turbo", temperature)

    elif provider == "ollama":
        # pip install langchain-community í•„ìš”
        # Ollama ë¡œì»¬ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨
        try:
            from langchain_community.llms import Ollama
            return Ollama(
                model=model_info["model"],
                temperature=temperature
            )
        except ImportError:
            print("âš ï¸ langchain-communityê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜: pip install langchain-community")
            return get_llm("gpt-3.5-turbo", temperature)
        except Exception as e:
            print(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            print("Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
            return get_llm("gpt-3.5-turbo", temperature)

    else:
        print(f"âš ï¸ {provider}ëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µìì…ë‹ˆë‹¤.")
        return get_llm("gpt-3.5-turbo", temperature)


# ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•œ ëª¨ë¸ ì„ íƒ í—¬í¼
def get_model_for_task(task_type="general"):
    """
    ì‘ì—… ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ

    Args:
        task_type: ì‘ì—… ìœ í˜• (research, writing, editing, seo, general)

    Returns:
        ëª¨ë¸ ì´ë¦„
    """

    # ì‘ì—…ë³„ ê¶Œì¥ ëª¨ë¸
    task_models = {
        "research": "gpt-oss",  # ë¦¬ì„œì¹˜ëŠ” ë¡œì»¬ ëª¨ë¸ë¡œ ë¬´ë£Œ!
        "writing": "gpt-oss",   # ì‘ì„±ë„ ë¡œì»¬ ëª¨ë¸
        "editing": "gpt-oss",   # í¸ì§‘ë„ ë¡œì»¬ ëª¨ë¸
        "seo": "gpt-oss",       # SEOë„ ë¡œì»¬ ëª¨ë¸
        "general": "gpt-oss"    # ì¼ë°˜ ì‘ì—…ë„ ë¡œì»¬
    }

    # í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
    override = os.getenv(f"MODEL_{task_type.upper()}")
    if override:
        return override

    return task_models.get(task_type, "gpt-3.5-turbo")


# ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ
def use_local_model():
    """
    ë¬´ë£Œ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ
    Ollama ì„¤ì¹˜ í•„ìš”: https://ollama.ai
    """

    print("ğŸ  ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ")
    print("-" * 50)
    print("1. Ollama ì„¤ì¹˜: curl -fsSL https://ollama.ai/install.sh | sh")
    print("2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama2")
    print("3. ì„œë²„ ì‹œì‘: ollama serve")
    print("4. ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("-" * 50)

    # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
    llm = get_llm("llama2", temperature=0.7)

    # í…ŒìŠ¤íŠ¸
    from langchain_core.messages import HumanMessage
    response = llm.invoke([
        HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ìê¸°ì†Œê°œí•´ì£¼ì„¸ìš”.")
    ])

    print(f"ì‘ë‹µ: {response.content}")
    return llm


if __name__ == "__main__":
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¶œë ¥
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
    print("-" * 50)
    for name, info in MODELS.items():
        print(f"â€¢ {name:15} | ì œê³µì: {info['provider']:10} | ë¹„ìš©: {info['cost']:6} | Function: {info['supports_functions']}")

    print("\nğŸ’¡ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ:")
    print("-" * 50)

    # ë‹¤ì–‘í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models_to_test = ["gpt-3.5-turbo", "gpt-4o-mini"]

    for model_name in models_to_test:
        print(f"\ní…ŒìŠ¤íŠ¸: {model_name}")
        llm = get_llm(model_name, temperature=0.5)
        print(f"âœ… {model_name} ë¡œë“œ ì„±ê³µ")