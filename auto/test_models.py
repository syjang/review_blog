"""
ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ LangGraph í…ŒìŠ¤íŠ¸
LangGraphëŠ” Function callingì´ ì—†ì–´ë„ ì‘ë™í•©ë‹ˆë‹¤!
"""

from config import get_llm, get_model_for_task, MODELS
from langgraph.graph import StateGraph, END
from typing import TypedDict
import os
from dotenv import load_dotenv

load_dotenv()


class SimpleState(TypedDict):
    """ê°„ë‹¨í•œ ìƒíƒœ ì •ì˜"""
    input: str
    output: str


def simple_workflow_test(model_name="gpt-3.5-turbo"):
    """
    ê°„ë‹¨í•œ ì›Œí¬í”Œë¡œìš°ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    Function calling ì—†ì´ë„ ì‘ë™í•©ë‹ˆë‹¤!
    """

    print(f"\n{'='*60}")
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model_name}")
    print(f"{'='*60}")

    # ëª¨ë¸ ì •ë³´
    model_info = MODELS.get(model_name, {})
    print(f"ì œê³µì: {model_info.get('provider', 'unknown')}")
    print(f"Function ì§€ì›: {model_info.get('supports_functions', False)}")
    print(f"ë¹„ìš©: {model_info.get('cost', 'unknown')}")
    print("-" * 60)

    # LLM ê°€ì ¸ì˜¤ê¸°
    try:
        llm = get_llm(model_name, temperature=0.5)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # ê°„ë‹¨í•œ ë…¸ë“œ í•¨ìˆ˜
    def process_node(state: SimpleState) -> SimpleState:
        """ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ"""
        from langchain_core.messages import HumanMessage, SystemMessage

        try:
            response = llm.invoke([
                SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
                HumanMessage(content=f"ë‹¤ìŒ ì œí’ˆì˜ ê°„ë‹¨í•œ íŠ¹ì§•ì„ 2ì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {state['input']}")
            ])

            # responseê°€ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ, ì•„ë‹ˆë©´ content ì¶”ì¶œ
            if hasattr(response, 'content'):
                state['output'] = response.content
            else:
                state['output'] = str(response)

            return state
        except Exception as e:
            print(f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            state['output'] = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            return state

    # LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
    workflow = StateGraph(SimpleState)
    workflow.add_node("process", process_node)
    workflow.set_entry_point("process")
    workflow.add_edge("process", END)

    # ì»´íŒŒì¼
    app = workflow.compile()

    # ì‹¤í–‰
    test_input = "ì• í”Œì›Œì¹˜"
    initial_state = {
        "input": test_input,
        "output": ""
    }

    print(f"ì…ë ¥: {test_input}")
    print("-" * 40)

    try:
        result = app.invoke(initial_state)
        print(f"ì¶œë ¥: {result['output']}")
        print(f"âœ… {model_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return False


def test_cost_optimization():
    """
    ë¹„ìš© ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸
    """
    print(f"\n{'='*60}")
    print("ğŸ’° ì‘ì—…ë³„ ìµœì  ëª¨ë¸ ì„ íƒ")
    print(f"{'='*60}")

    tasks = ["research", "writing", "editing", "seo", "general"]

    for task in tasks:
        model = get_model_for_task(task)
        model_info = MODELS.get(model, {})
        print(f"â€¢ {task:10} â†’ {model:15} (ë¹„ìš©: {model_info.get('cost', 'unknown')})")


def test_free_models():
    """
    ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ
    """
    print(f"\n{'='*60}")
    print("ğŸ†“ ë¬´ë£Œ/ì €ë ´í•œ ëª¨ë¸ ì˜µì…˜")
    print(f"{'='*60}")

    free_models = [
        ("gpt-3.5-turbo", "ì €ë ´í•œ OpenAI ëª¨ë¸"),
        ("llama2", "ë¬´ë£Œ ë¡œì»¬ ëª¨ë¸ (Ollama)"),
        ("mistral", "ë¬´ë£Œ ë¡œì»¬ ëª¨ë¸ (Ollama)"),
        ("gemma", "ë¬´ë£Œ ë¡œì»¬ ëª¨ë¸ (Ollama)")
    ]

    for model, desc in free_models:
        model_info = MODELS.get(model, {})
        print(f"\nğŸ“Œ {model}")
        print(f"   ì„¤ëª…: {desc}")
        print(f"   ì œê³µì: {model_info.get('provider', 'unknown')}")
        print(f"   ë¹„ìš©: {model_info.get('cost', 'unknown')}")

        if model_info.get('provider') == 'ollama':
            print(f"   ğŸ’¡ íŒ: Ollama ì„¤ì¹˜ í›„ 'ollama pull {model_info.get('model')}' ì‹¤í–‰")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("""
    ğŸ¤– LangGraph ëª¨ë¸ ìœ ì—°ì„± í…ŒìŠ¤íŠ¸
    ================================

    LangGraphëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
    Function callingì´ ë°˜ë“œì‹œ í•„ìš”í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.
    """)

    # 1. ë¹„ìš© ìµœì í™” í…ŒìŠ¤íŠ¸
    test_cost_optimization()

    # 2. ë¬´ë£Œ ëª¨ë¸ ì˜µì…˜ í‘œì‹œ
    test_free_models()

    # 3. ì‹¤ì œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print(f"\n{'='*60}")
    print("ğŸš€ ì‹¤ì œ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print(f"{'='*60}")

    # API í‚¤ê°€ ìˆëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸
    if os.getenv("OPENAI_API_KEY"):
        # ì €ë ´í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        simple_workflow_test("gpt-3.5-turbo")
    else:
        print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ëŒ€ì•ˆ: Ollama ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”!")
        print("\nì„¤ì¹˜ ë°©ë²•:")
        print("1. Ollama ì„¤ì¹˜: curl -fsSL https://ollama.ai/install.sh | sh")
        print("2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama2")
        print("3. ì„œë²„ ì‹œì‘: ollama serve")
        print("4. ì´ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰")

    print(f"\n{'='*60}")
    print("ğŸ“ ê²°ë¡ :")
    print("â€¢ LangGraphëŠ” Function calling ì—†ì´ë„ ì‘ë™í•©ë‹ˆë‹¤")
    print("â€¢ GPT-3.5-turbo ê°™ì€ ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
    print("â€¢ Ollamaë¡œ ì™„ì „ ë¬´ë£Œ ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥")
    print("â€¢ ì‘ì—…ë³„ë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ ë¹„ìš© ìµœì í™” ê°€ëŠ¥")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()