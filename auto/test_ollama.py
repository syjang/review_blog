"""
Ollama ë¡œì»¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸
ì™„ì „ ë¬´ë£Œë¡œ LangGraph ì‚¬ìš©í•˜ê¸°!
"""

from config import get_llm
from langchain_core.messages import HumanMessage, SystemMessage

def test_ollama():
    """Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Ollama gpt-oss:20b ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)

    # gpt-oss ëª¨ë¸ ì‚¬ìš©
    llm = get_llm("gpt-oss", temperature=0.7)

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë¦¬ë·° ì‘ì„±ìì…ë‹ˆë‹¤."),
        HumanMessage(content="ì• í”Œì›Œì¹˜ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”.")
    ]

    print("ğŸ“ ìš”ì²­: ì• í”Œì›Œì¹˜ì˜ ì¥ì  3ê°€ì§€")
    print("-"*40)

    try:
        response = llm.invoke(messages)
        print("âœ… ì‘ë‹µ:")
        print(response.content if hasattr(response, 'content') else response)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_ollama()